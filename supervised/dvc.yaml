vars:  
  - aws:
      account_id: "644482713970"
      region_name: "eu-north-1"
      role_name: "pg2-benchmark-sagemaker-role"
      s3_output_bucket: "pg2-benchmark-output"
      s3_training_data_bucket: "pg2-benchmark-training-data"
      instance_type: "ml.m5.xlarge"
      volume_size: 5

  - local:
      data_dir: data
      model_dir: model
      output_dir: output/aws
      metric_dir: metric/aws
  
  - datasets:
    - name: charge_ladder
      path: data/dummy/charge_ladder.toml
  
    - name: neime
      path: data/neime/neime.toml

  - models:
    - name: pls
      path: model/pls.toml
      dockerfile: ../models/pls

stages:
  upload_to_s3:
    cmd:
      - aws s3 mb s3://${aws.s3_training_data_bucket} --region ${aws.region_name} || true
      - aws s3 mb s3://${aws.s3_output_bucket} --region ${aws.region_name} || true
      - aws s3 cp ${local.data_dir}/ s3://${aws.s3_training_data_bucket}/${local.data_dir}/ --recursive --exclude ".*" --exclude "*/.*"
      - aws s3 cp ${local.model_dir}/ s3://${aws.s3_training_data_bucket}/${local.model_dir}/ --recursive --exclude ".*" --exclude "*/.*"
      - echo "Upload completed at $(date)" > logs/s3_upload_complete.txt
    deps:
      - ${local.data_dir}/
      - ${local.model_dir}/
    outs:
      - logs/s3_upload_complete.txt:
          cache: true
  
  deploy_to_ecr:
    matrix:
      model: ${models}
    cmd:
      - aws ecr describe-repositories --repository-names ${item.model.name} --region ${aws.region_name} >/dev/null 2>&1 || aws ecr create-repository --repository-name ${item.model.name} --region ${aws.region_name} >/dev/null
      - aws ecr get-login-password --region ${aws.region_name} | docker login --username AWS --password-stdin ${aws.account_id}.dkr.ecr.${aws.region_name}.amazonaws.com
      - docker buildx build --platform linux/amd64,linux/arm64 --secret id=git_auth,src=git-auth.txt -t ${aws.account_id}.dkr.ecr.${aws.region_name}.amazonaws.com/${item.model.name}:latest ${item.model.dockerfile} --no-cache --push
      - echo "ECR push completed at $(date)" > logs/ecr_push_complete.txt
      - echo "${aws.account_id}.dkr.ecr.${aws.region_name}.amazonaws.com/${item.model.name}:latest" > logs/image_uri.txt
    deps:
      - ${item.model.dockerfile}
    outs:
      - logs/ecr_push_complete.txt:
          cache: true
      - logs/image_uri.txt:
          cache: true
  
  create_role:
    cmd:
      - aws iam get-role --role-name ${aws.role_name} >/dev/null 2>&1 || aws iam create-role --role-name ${aws.role_name} --assume-role-policy-document file://trust-policy.json >/dev/null
      - aws iam attach-role-policy --role-name ${aws.role_name} --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
      - aws iam attach-role-policy --role-name ${aws.role_name} --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
      - echo "SageMaker role created at $(date)" > logs/role_create_complete.txt
    deps:
      - logs/s3_upload_complete.txt
      - logs/ecr_push_complete.txt
      - logs/image_uri.txt
    outs:
      - logs/role_create_complete.txt:
          cache: true


  create_training_job:
    matrix: 
      dataset: ${datasets}
      model: ${models}
    
    cmd: >
      uv run pg2-benchmark aws create-training-job
      --model-name ${item.model.name}
      --region-name ${aws.region_name}
      --role-name ${aws.role_name}
      --ecr-repository-uri ${aws.account_id}.dkr.ecr.${aws.region_name}.amazonaws.com/${item.model.name}
      --s3-training-data-bucket ${aws.s3_training_data_bucket}
      --s3-output-bucket ${aws.s3_output_bucket}
      --instance-type ${aws.instance_type}
      --volume-size ${aws.volume_size}
      --dataset-toml-file ${item.dataset.path}
      --model-toml-file ${item.model.path}
      > logs/create_job_${item.dataset.name}_${item.model.name}.txt

    deps:
      - ../src/pg2_benchmark/cli/aws.py
      - logs/role_create_complete.txt
    
    outs:
      - logs/create_job_${item.dataset.name}_${item.model.name}.txt:
          cache: true
  
  monitor_training_job:
    matrix: 
      dataset: ${datasets}
      model: ${models}
    
    cmd: >
      uv run pg2-benchmark aws monitor-training-job
      --region-name ${aws.region_name}
      --job-name $(cat logs/create_job_${item.dataset.name}_${item.model.name}.txt)
      > logs/monitor_job_${item.dataset.name}_${item.model.name}.txt
    
    deps:
      - ../src/pg2_benchmark/cli/aws.py
      - logs/create_job_${item.dataset.name}_${item.model.name}.txt
    
    outs:
     - logs/monitor_job_${item.dataset.name}_${item.model.name}.txt:
          cache: true

  calculate_metric:
    matrix: 
      dataset: ${datasets}
      model: ${models}
    
    cmd:
      - aws s3 cp s3://${aws.s3_output_bucket}/$(cat logs/create_job_${item.dataset.name}_${item.model.name}.txt)/${local.output_dir}/model.tar.gz ${local.output_dir}/
      - tar -xzf ${local.output_dir}/model.tar.gz -C ${local.output_dir}/
      - rm ${local.output_dir}/model.tar.gz
      - uv run pg2-benchmark metric calc --output-path ${local.output_dir}/${item.dataset.name}_${item.model.name}.csv --metric-path ${local.metric_dir}/${item.dataset.name}_${item.model.name}.csv

    deps:
      - logs/monitor_job_${item.dataset.name}_${item.model.name}.txt
    outs:
      - ${local.metric_dir}/${item.dataset.name}_${item.model.name}.csv:
          cache: true