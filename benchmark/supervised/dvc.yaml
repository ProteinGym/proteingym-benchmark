vars:
  - default.yaml
  - datasets.json
  - models.json

stages:

  create_training_job:
    matrix:
      dataset: ${datasets}
      model: ${models}

    cmd: >-
      docker run --rm
      -v $(realpath ${item.dataset.input_filename}):/$(basename ${item.dataset.input_filename})
      -v $(realpath ${output.prediction}):/opt/program/output
      ${item.model.image}
      train --dataset-file /$(basename ${item.dataset.input_filename})
    deps:
      - ../../models
      - ${item.dataset.input_filename}
    outs:
      - ${output.prediction}/${item.dataset.name}_${item.model.name}.csv:
          cache: true

  calculate_metric:
    matrix:
      dataset: ${datasets}
      model: ${models}

    cmd:      
      - >- 
        METRICS=$(python -c "import yaml; data = yaml.safe_load(open('default.yaml')); print(' '.join(data['metrics']))") &&
        python $(dvc root)/scripts/metric.py
        --prediction-path ${output.prediction}/${item.dataset.name}_${item.model.name}.csv
        --metric-path ${output.metric}/${item.dataset.name}_${item.model.name}.json
        --selected-metrics $METRICS

    deps:
      - ../../scripts/metric.py
      - ${output.prediction}/${item.dataset.name}_${item.model.name}.csv
    metrics:
      - ${output.metric}/${item.dataset.name}_${item.model.name}.json:
          cache: false
