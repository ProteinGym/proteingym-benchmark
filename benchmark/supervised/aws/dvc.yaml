vars:
  - default.yaml
  - datasets.json
  - models.json

stages:

  setup:
    cmd:
      - mkdir -p logs ${output.prediction} ${output.metric}
      - echo "Created local directories" > logs/setup.txt
    outs:
      - logs/setup.txt
  
  upload_datasets_to_s3:
    matrix:
      dataset: ${datasets}

    cmd:
      - aws s3 cp ${item.dataset.input_filename} s3://${aws.s3_training_data_prefix}/datasets/${item.dataset.name}/dataset.pgdata
      - echo "Dataset upload completed at $(date)" > logs/s3_upload_dataset_${item.dataset.name}.txt
    deps:
      - logs/setup.txt
      - ${item.dataset.input_filename}
    outs:
      - logs/s3_upload_dataset_${item.dataset.name}.txt:
          cache: true

  upload_models_to_s3:
    matrix:
      model: ${models}

    cmd:
      - aws s3 cp ${item.model.input_filename} s3://${aws.s3_training_data_prefix}/models/${item.model.name}/README.md
      - echo "Model upload completed at $(date)" > logs/s3_upload_model_${item.model.name}.txt
    deps:
      - logs/setup.txt
      - ${item.model.input_filename}
    outs:
      - logs/s3_upload_model_${item.model.name}.txt:
          cache: true
  
  deploy_to_ecr:
    matrix:
        model: ${models}
    cmd:
      - >-
        aws ecr describe-repositories
        --repository-names ${item.model.name}
        --region ${aws.region_name} >/dev/null 2>&1 || aws ecr create-repository
        --repository-name ${item.model.name}
        --region ${aws.region_name} >/dev/null
      - >-
        aws ecr get-login-password
        --region ${aws.region_name} | docker login
        --username AWS
        --password-stdin ${aws.account_id}.dkr.ecr.${aws.region_name}.amazonaws.com
      - >- 
        docker buildx build
        --platform linux/amd64,linux/arm64
        -f $(dirname ${item.model.input_filename})/Dockerfile
        -t ${aws.account_id}.dkr.ecr.${aws.region_name}.amazonaws.com/${item.model.name}:latest
        $(dirname ${item.model.input_filename})
        --push
      - echo "ECR push completed at $(date)" > logs/ecr_push_${item.model.name}.txt
      - echo "${aws.account_id}.dkr.ecr.${aws.region_name}.amazonaws.com/${item.model.name}:latest" > logs/image_uri_${item.model.name}.txt
    deps:
      - logs/setup.txt
      - ${item.model.input_filename}
    outs:
      - logs/ecr_push_${item.model.name}.txt:
          cache: true
      - logs/image_uri_${item.model.name}.txt:
          cache: true

  create_training_job:
    matrix:
      dataset: ${datasets}
      model: ${models}
    
    cmd: >
      python $(dvc root)/scripts/sagemaker.py create
      --dataset-name ${item.dataset.name}
      --model-name ${item.model.name}
      --region-name ${aws.region_name}
      --sagemaker-role-name ${aws.sagemaker_role_name}
      --ecr-repository-uri ${aws.account_id}.dkr.ecr.${aws.region_name}.amazonaws.com/${item.model.name}
      --s3-training-data-prefix ${aws.s3_training_data_prefix}
      --s3-output-prefix ${aws.s3_output_prefix}
      --instance-type ${aws.instance_type}
      --volume-size ${aws.volume_size}
      > logs/create_job_${item.dataset.name}_${item.model.name}.txt

    deps:
      - ../../../scripts/sagemaker.py
      - logs/s3_upload_dataset_${item.dataset.name}.txt
      - logs/s3_upload_model_${item.model.name}.txt
      - logs/ecr_push_${item.model.name}.txt
      - logs/image_uri_${item.model.name}.txt
    
    outs:
      - logs/create_job_${item.dataset.name}_${item.model.name}.txt:
          cache: true
  
  monitor_training_job:
    matrix: 
      dataset: ${datasets}
      model: ${models}
    
    cmd: >
      python $(dvc root)/scripts/sagemaker.py monitor
      --region-name ${aws.region_name}
      --job-name $(cat logs/create_job_${item.dataset.name}_${item.model.name}.txt)
      --poll-interval 30
      --timeout 3600
      > logs/monitor_job_${item.dataset.name}_${item.model.name}.txt
    
    deps:
      - ../../../scripts/sagemaker.py
      - logs/create_job_${item.dataset.name}_${item.model.name}.txt
    
    outs:
     - logs/monitor_job_${item.dataset.name}_${item.model.name}.txt:
          cache: true

  calculate_metric:
    matrix: 
      dataset: ${datasets}
      model: ${models}
    
    cmd:
      - >-
        aws s3 cp
        s3://${aws.s3_output_prefix}/$(cat logs/create_job_${item.dataset.name}_${item.model.name}.txt)/output/model.tar.gz
        ${output.prediction}/
      - tar -xzf ${output.prediction}/model.tar.gz -C ${output.prediction}/
      - rm ${output.prediction}/model.tar.gz
      - >-
        python $(dvc root)/scripts/metric.py "test" "pred"
        --prediction-path ${output.prediction}/${item.dataset.name}_${item.model.name}.csv
        --metric-path ${output.metric}/${item.dataset.name}_${item.model.name}.json
        --selected-metrics ${metrics}

    deps:
      - logs/monitor_job_${item.dataset.name}_${item.model.name}.txt
    metrics:
      - ${output.metric}/${item.dataset.name}_${item.model.name}.json:
          cache: false
